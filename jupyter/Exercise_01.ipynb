{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c2b7ff-f6d4-4fea-b022-e892738bc871",
   "metadata": {},
   "source": [
    "# Exercise 01: Gradient Descent\n",
    "\n",
    "**Learn a bit about gradient descent from videos, web pages, books, etc., and summarize some key concepts below.  Cite your sources.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a1218c-fce4-4ed7-9802-452d2ee55dc2",
   "metadata": {},
   "source": [
    "<font color='red'>Gradient descent is an iterative algorithm to find a local minimum.  It does this by going in the opposite direction of the local gradient.  One might ask, why don't we plot the function and just select the gradient almost by inspection?  If the function is a result of more than 2 variables then you cannot plot it in a 3D coordinate system and therefore cannot find the minimum by inspection.  Instead, taking the gradient $\\nabla F(x,y,...)$ and subtracting it from the current step, you will move towards the local minimum $\\theta^1 = \\theta^0 - \\eta \\nabla f(\\theta^0)$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e6561-768a-43a5-a734-dbd16dd3cae8",
   "metadata": {},
   "source": [
    "Sources: \n",
    "- https://www.youtube.com/watch?v=qg4PchTECck\n",
    "- https://en.wikipedia.org/wiki/Gradient_descent\n",
    "- https://www.youtube.com/watch?v=sDv4f4s2SB8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
